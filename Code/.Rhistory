numericInput('mean_PIapp', 'mean_PIratio_approved', mean(loanDf$PIratio[which(loanDf$target == 0)]), min = 0, max = 25),
numericInput('mean_PIdeni', 'mean_PIratio_denied',mean(loanDf$PIratio[which(loanDf$target == 1)]) , min = 0, max = 25),
numericInput('sd_solapp', 'sd_solvency_approved', sd(loanDf$solvency[which(loanDf$target == 0)]), min=0, max=40),
numericInput('sd_soldeni', 'sd_solvency_denied', sd(loanDf$solvency[which(loanDf$target == 1)]), min=0, max=40),
numericInput('sd_PIapp', 'sd_PIratio_approved', sd(loanDf$PIratio[which(loanDf$target == 0)]), min = 0, max = 10),
numericInput('sd_PIdeni', 'sd_PIratio_denied', sd(loanDf$PIratio[which(loanDf$target == 1)]), min = 0, max = 10)
),
mainPanel(
plotOutput('plot1'),
plotOutput('contingency_matrix')
)
)
)
sigmaXY <- function(rho, sdX, sdY) {
covTerm <- rho * sdX * sdY
VCmatrix <- matrix(c(sdX^2, covTerm, covTerm, sdY^2),
2, 2, byrow = TRUE)
return(VCmatrix)
}
genBVN <- function(n = 1, seed = NA, muXY=c(0,1), sigmaXY=diag(2)) {
if(!is.na(seed)) set.seed(seed)
rdraws <- rmvnorm(n, mean = muXY, sigma = sigmaXY)
return(rdraws)
}
# correlation is slightly negative
sigmaXY(rho=-0.1, sdX=1, sdY=20)
# highly positive
sigmaXY(rho=0.8, sdX=2, sdY=30)
# creating a function for all of this
loanData <- function(noApproved, noDenied, muApproved, muDenied, sdApproved,
sdDenied, rhoApproved, rhoDenied, seed=1111) {
sigmaApproved <- sigmaXY(rho=rhoApproved, sdX=sdApproved[1], sdY=sdApproved[2])
sigmaDenied <- sigmaXY(rho=rhoDenied, sdX=sdDenied[1], sdY=sdDenied[2])
approved <- genBVN(noApproved, muApproved, sigmaApproved, seed = seed)
denied <- genBVN(noDenied, muDenied, sigmaDenied, seed = seed+1)
loanDf <- as.data.frame(rbind(approved,denied))
deny <- c(rep("Approved", noApproved), rep("Denied", noDenied))
target = c(rep(0, noApproved), rep(1, noDenied))
loanDf <- data.frame(loanDf, deny, target)
colnames(loanDf) <- c("PIratio", "solvency", "deny", "target")
return(loanDf)
}
# generating some data
loanDf <- loanData(noApproved=50, noDenied=50, c(4, 150), c(10, 100),
c(1,20), c(2,30), -0.1, 0.6, 1221)
mean(loanDf$solvency[which(loanDf$target == 0)])
mean(loanDf$solvency[which(loanDf$target == 1)])
mean(loanDf$PIratio[which(loanDf$target == 0)])
mean(loanDf$PIratio[which(loanDf$target == 1)])
sd(loanDf$solvency[which(loanDf$target == 0)])
sd(loanDf$solvency[which(loanDf$target == 1)])
sd(loanDf$PIratio[which(loanDf$target == 0)])
sd(loanDf$PIratio[which(loanDf$target == 1)])
library(ggplot2)
library(mvtnorm)
library(shiny)
shinyServer(function(input, output, session) {
# create small wrapper functions
sigmaXY <- function(rho, sdX, sdY) {
covTerm <- rho * sdX * sdY
VCmatrix <- matrix(c(sdX^2, covTerm, covTerm, sdY^2),
2, 2, byrow = TRUE)
return(VCmatrix)
}
genBVN <- function(n = 1, seed = NA, muXY=c(0,1), sigmaXY=diag(2)) {
if(!is.na(seed)) set.seed(seed)
rdraws <- rmvnorm(n, mean = muXY, sigma = sigmaXY)
return(rdraws)
}
# creating a function for all of this
loanData <- function(noApproved, noDenied, muApproved, muDenied, sdApproved,
sdDenied, rhoApproved, rhoDenied, seed=1111) {
sigmaApproved <- sigmaXY(rho=rhoApproved, sdX=sdApproved[1], sdY=sdApproved[2])
sigmaDenied <- sigmaXY(rho=rhoDenied, sdX=sdDenied[1], sdY=sdDenied[2])
approved <- genBVN(noApproved, muApproved, sigmaApproved, seed = seed)
denied <- genBVN(noDenied, muDenied, sigmaDenied, seed = seed+1)
loanDf <- as.data.frame(rbind(approved,denied))
deny <- c(rep("Approved", noApproved), rep("Denied", noDenied))
target = c(rep(0, noApproved), rep(1, noDenied))
loanDf <- data.frame(loanDf, deny, target)
colnames(loanDf) <- c("PIratio", "solvency", "deny", "target")
return(loanDf)
}
# Combine the selected variables into a new data frame
selectedData <- reactive({
select <- loanData(noApproved=50, noDenied=50, c(input$mean_solapp, input$mean_PIapp), c(input$mean_soldeni, input$mean_PIdeni),
c(input$sd_solapp, input$sd_PIapp), c(input$sd_soldeni, input$sd_solapp), -0.1, 0.6, 1221)
return(select)
})
##update regression line
datafit <- reactive({
modelfit <- lm(target ~ solvency + PIratio + 1, data=selectedData())
weights <- coef(modelfit)[c("solvency", "PIratio")]
bias <- coef(modelfit)[1]
# Computing the boundary: since it is a 2-dimensional example the boundary
# is a line.
intercept <- (-bias + 0.5)/weights["PIratio"]
slope <- -(weights["solvency"]/weights["PIratio"])
predictedLabels <- ifelse(predict(datafit) < 0.5, "Approved", "Denied")
list(intercept = intercept, slope = slope, predictedLabels = predictedLabels)
})
output$plot1 <- renderPlot({
par(mar = c(5.1, 4.1, 0, 1))
ggplot(data = selectedData(), aes(x = solvency, y = PIratio,
colour=deny, fill=deny)) +
geom_point() +
xlab("solvency") +
ylab("PIratio") +
theme_bw() +
theme(text=element_text(family="Arial")) +
geom_abline(intercept = datafit$intercept, slope = datafit$slope)
})
output$contingency_matrix <- renderDataTable({
confMatrixFreq <- table(selectedData()$deny, datafit$predictedLabels)
confMatrixFreq
})
})
---
title: "Advanced computational methods - Lecture 1"
author: "Hrvoje Stojic"
date: "January 8, 2016"
output:
html_document:
highlight: kate
theme: default
number_sections: yes
pdf_document:
fig_caption: no
highlight: kate
keep_tex: no
number_sections: yes
latex_engine: xelatex
fontsize: 12pt
documentclass: article
sansfont: Roboto
---
```{r, knitr_options, cache=TRUE,  include=FALSE}
# loading in required packages
if (!require("knitr")) install.packages("knitr"); library(knitr)
if (!require("rmarkdown")) install.packages("rmarkdown"); library(rmarkdown)
# some useful global defaults
opts_chunk$set(comment='##', warning=FALSE, message=FALSE, include=TRUE, echo=TRUE, cache=TRUE, cache.comments=FALSE)
# output specific defaults
output <- opts_knit$get("rmarkdown.pandoc.to")
if (output=="html") opts_chunk$set(fig.width=10, fig.height=5)
if (output=="latex") opts_chunk$set(fig.width=6,  fig.height=4)
```
```{r, Setup_and_Loading_Data, include=FALSE}
# cleaning before starting, for interactive session
# rm(list=ls())
# setwd("/home/hrvoje/Cloud/Teaching/BGSE_DS_AdvComputing_2016/Lecture1_discFunctions/")
# loading in required packages
if (!require("mvtnorm")) install.packages("mvtnorm"); library(mvtnorm)
if (!require("ggplot2")) install.packages("ggplot2"); library(ggplot2)
```
# Introduction
This session introduces basic parametric method for classification that relies on linear regression.
## Using .Rmd documents
In the files for this session you will find a source file of this document with .Rmd extension. To use these files simply open them as you would open any other R script. Rmd stands for rmarkdown, a combination of R code and lowly formatted text (this is why it is called mark*down*).
To be able to build PDF, HTML and other types of documents from it you need to install [Rmarkdown](http://rmarkdown.rstudio.com/) and [knitr](http://yihui.name/knitr/) packages. If you use RStudio there are buttons that build documents automatically, while if you are using a text editor you will need to run a specific command. For example, open a terminal in the folder containing the .Rmd file and run `R -e 'library(knitr);library(rmarkdown);rmarkdown::render("file_name.Rmd", "html_document")'`. Note that PDF output (including Beamer slides) requires a full installation of TeX.
**Why knitr or Rmarkdown?**
- **Reproducibility**:
- It makes your data analysis more reproducible. The R code describes exactly the steps from the raw data to the final report. This makes it perfect for sharing reports with your colleagues.
- It is written with almost no formatting at all (markdown), which makes it easier to convert to any other format, from nicely looking PDFs to the all-present MS docx and complete HTML documents (fancy a blog?).
- **Efficiency**:
- Statistical output from figures to tables is automatically placed in your report. No more copy-pasting and reformatting the output from your statistical analysis program into your report.
- You want to use a slightly different subset of the data? You want to drop that outlier observation? No problem, you can update your report with a single click instead of updating every table and figure.
- Whoever has done some copy-pasting knows how easy is to overlook one number or one figure. This type of document also significantly reduces the chance of such errors.
- **Education & Communication**:
- Excellent for teaching as one can check how exactly is some analysis done from the report.
- Do not disregard this aspect, look at Github and Stackoverflow stars who get job offers on this account!
Note that I used a different font in the document than the default. To use different fonts in your documents, in the header of the .Rmd file, under pdf_document you need to set "latex_engine: xelatex", and then instead of "sansfont: Arial" use whatever font you have available to Tex system on your computer (usually whatever fonts are available on your computer).
--------
# Discriminant functions a.k.a. Linear probability models
## Simple categorization problem
To illustrate discriminant functions we will use a simple artificial dataset - two dimensional data with two categories - whether a loan in the bank will be denied or approved, with two observed variables, payment-to-income ratio and solvency score.
We will assume that bot variables normally distributed variables, not necessarily independent from each other. For more general m-dimensional case we can write it as follows:
$$ N(\mathbf{x}|\mathbf{\mu},\Sigma) = \frac{1}{ (2\pi)^{D/2} |\Sigma|^{1/2} } exp\{ \frac{1}{2}(\mathbf{x}-\mathbf{\mu})^T \Sigma^{-1} ( \mathbf{x} -\mathbf{\mu}) \} $$
where $\mathbf{x}$ is m-dimensional vector, $\Sigma$ is positive-definite covariance matrix and $\mathbf{\mu}$ is a vector of means. Since we have a 2-dimensional case our joint distribution is governed by the following parameters:
$$ \boldsymbol\mu = \begin{pmatrix} \mu_x \\ \mu_y \end{pmatrix}, \quad
\boldsymbol\Sigma = \begin{pmatrix} \sigma_x^2 & \rho \sigma_x \sigma_y \\
\rho \sigma_x \sigma_y  & \sigma_y^2 \end{pmatrix} $$
where $\rho$ is correlation between $X$ and $Y$.
We use the R package `mvtnorm` to draw (pseudo)-random numbers from such joint normal distribution (read the docs [here](http://cran.r-project.org/web/packages/mvtnorm/index.html)).
```{r, MVN_example_simple}
# simple example with uncorrelated variables
set.seed(7851)
x <- rmvnorm(n=100, mean=c(1,5), sigma=1*diag(2))
plot(x[, 1], x[, 2], col = "blue", xlab = "PIratio", ylab = "solvency")
title("PI ratio and solvency - drawn\nfrom bivariate normal distribution")
```
You might have noticed that sometimes figures produced by R when saved in PDF format do not display correctly. This is because it does not embed fonts into PDF when it is generated. PDF viewers and client systems that lack the font you have used will substitute it with some other font. Actually, Helvetica, default font used in R is not present in many open source PDF viewers and operating systems. You will usually want to make sure that your users will be able to see the figure correctly. Below I show you how to embed fonts via `extrafont` package (another alternative is `showtext` package). Moreover, with this package you can expand the number of fonts you can use in your figures.
```{r, MVN_example_simple_fonts}
# embedding fonts into our pdf figures
if (!require("extrafont")) install.packages("extrafont"); library(extrafont)
# importing fonts from your OS into R, need to be run only once
# font_import()
# show fonts available
# fonts()  # or fonttable()
# load the fonts into R session
loadfonts()
# you would then embed fonts as follows
pdf("plot_embedded_fonts.pdf", family="Arial", width=4, height=4.5)
plot(x[, 1], x[, 2], col = "blue", xlab = "PIratio", ylab = "solvency")
title("PI ratio and solvency - drawn\nfrom bivariate normal distribution")
dev.off()
embed_fonts("plot_embedded_fonts.pdf")
# however, rmarkdown does that automatically, how can we instruct it
# to embed fonts?
# 1. we need to change the device to cairo_pdf(), this device can
#    automatically embed fonts into PDF plots, normal pdf() cannot
# 2. we add dev.args family argument in the chunk specs
if (output=="latex") {
opts_chunk$set(dev = 'cairo_pdf',
dev.args=list(family="Arial"))
}
```
```{r, MVN_example_simple_fonts_cont}
# now plot the figure as usual
plot(x[, 1], x[, 2], col = "blue", xlab = "PIratio", ylab = "solvency")
title("PI ratio and solvency - drawn\nfrom bivariate normal distribution")
```
If you need to draw a *really* high dimensional data (>1000), drawing them with the help of `mvtnorm` package might be too slow. There are `Rcpp` implementations that are faster (see for an example [here](http://stackoverflow.com/questions/22738355/efficiently-radomly-drawing-from-a-multivariate-normal-distribution))
In the figure above PIratio and solvency were uncorrelated, but this is often not realistic. Let us use `mvtnorm` to draw samples for which covariance terms in the $\Sigma$ matrix are not zero.
```{r, MVN_example_corr}
# create small wrapper functions
sigmaXY <- function(rho, sdX, sdY) {
covTerm <- rho * sdX * sdY
VCmatrix <- matrix(c(sdX^2, covTerm, covTerm, sdY^2),
2, 2, byrow = TRUE)
return(VCmatrix)
}
genBVN <- function(n = 1, seed = NA, muXY=c(0,1), sigmaXY=diag(2)) {
if(!is.na(seed)) set.seed(seed)
rdraws <- rmvnorm(n, mean = muXY, sigma = sigmaXY)
return(rdraws)
}
# correlation is slightly negative
sigmaXY(rho=-0.1, sdX=1, sdY=20)
# highly positive
sigmaXY(rho=0.8, sdX=2, sdY=30)
# creating a function for all of this
loanData <- function(noApproved, noDenied, muApproved, muDenied, sdApproved,
sdDenied, rhoApproved, rhoDenied, seed=1111) {
sigmaApproved <- sigmaXY(rho=rhoApproved, sdX=sdApproved[1], sdY=sdApproved[2])
sigmaDenied <- sigmaXY(rho=rhoDenied, sdX=sdDenied[1], sdY=sdDenied[2])
approved <- genBVN(noApproved, muApproved, sigmaApproved, seed = seed)
denied <- genBVN(noDenied, muDenied, sigmaDenied, seed = seed+1)
loanDf <- as.data.frame(rbind(approved,denied))
deny <- c(rep("Approved", noApproved), rep("Denied", noDenied))
target = c(rep(0, noApproved), rep(1, noDenied))
loanDf <- data.frame(loanDf, deny, target)
colnames(loanDf) <- c("PIratio", "solvency", "deny", "target")
return(loanDf)
}
loanDf <- loanData(noApproved=50, noDenied=50, c(4, 150), c(10, 100),
c(1,20), c(2,30), -0.1, 0.6, 1221)
View(loanDf)
library(extrafont)
loadfonts()
library(class)
train <- read.csv("../DATA/news_popularity_training.csv", sep = ",")
#tables
table(train$popularity)
round(prop.table(table(train$popularity)) * 100, digits = 1)
#normalize
normalize <- function(x) {
num <- x - min(x)
denom <- max(x) - min(x)
return (num/denom)
}
train_norm <- as.data.frame(lapply(train[4:61], normalize))
train_norm <- cbind(train[1:3],train_norm,train$popularity)
?sample
#Sample
ind <- sample(2, nrow(train), replace=TRUE, prob=c(0.67, 0.33))
train.training <- train[ind==1, 1:61]
train.test <- train[ind==2, 1:61]
train.trainLabels <- train[ind==1, 62]
train.testLabels <- train[ind==2, 62]
#kkn
train_pred <- knn(train = train.training, test = train.test, cl = train.trainLabels, k=3)
?knn
iris3[,,2]
setwd("~/Scrivania/BGSE/Winter_term/Kaggle_Competition/Code")
train <- read.csv("../DATA/news_popularity_training.csv", sep = ",")
#tables
table(train$popularity)
round(prop.table(table(train$popularity)) * 100, digits = 1)
#normalize
normalize <- function(x) {
num <- x - min(x)
denom <- max(x) - min(x)
return (num/denom)
}
train_norm <- as.data.frame(lapply(train[4:61], normalize))
train_norm <- cbind(train[1:3],train_norm,train$popularity)
?sample
#Sample
ind <- sample(2, nrow(train), replace=TRUE, prob=c(0.67, 0.33))
train.training <- train[ind==1, 1:61]
train.test <- train[ind==2, 1:61]
train.trainLabels <- train[ind==1, 62]
train.testLabels <- train[ind==2, 62]
#kkn
train_pred <- knn(train = train.training, test = train.test, cl = train.trainLabels, k=3)
?knn
iris3[,,2]
train_pred <- knn(train = train.training, test = train.test, cl = train.trainLabels, k=3)
install.packages("kknn")
library(kknn)
?kknn
train_pred <- knn(train = train.training, test = train.test, cl = train.trainLabels, k=3, na.action = na.omit())
train_pred1 <- kknn(formula = formula(train.training), train.training, train.test, na.action=na.omit(), k =5)
train_pred1 <- kknn() train.training, train.test, na.action=na.omit(), k =5)
train_pred1 <- kknn( train.training, train.test, na.action=na.omit(), k =5)
train_pred <- knn(train = train.training, test = train.test, cl = train.trainLabels, k=3)
View(train)
ind <- sample(2, nrow(train), replace=TRUE, prob=c(0.67, 0.33))
train.training <- train[ind==1, 3:61]
train.test <- train[ind==2, 3:61]
train.trainLabels <- train[ind==1, 62]
train.testLabels <- train[ind==2, 62]
train_pred <- knn(train = train.training, test = train.test, cl = train.trainLabels, k=3)
train_pred1 <- kknn( train.training, train.test, na.action=na.omit(), k =5)
train.test <- train[ind==2, 3:61]
train.trainLabels <- train[ind==1, 62]
train.testLabels <- train[ind==2, 62]
train_pred1 <- kknn( train.training, train.test, na.action=na.omit(), k =5)
train_pred1 <- kknn(formula= formula(train.training), train.training, train.test, na.action=na.omit(), k =5)
train_pred2 <- train_pred1$fitted.values
train_pred2 <- train_pred1$response
train_pred2 <- train_pred1$prob
train_pred <- knn(train = train.training, test = train.test, cl = train.trainLabels, k=5)
train_pred
c(1:5)
cbind(train_pred, train.test)
train_pred <- as.data.fram(train_pred)
train_pred <- as.data.frame(train_pred)
cbind(train_pred, train.testLabels)
result <- cbind(train_pred, train.testLabels)
View(result)
sum(result$train_pred == train.testLabels)
sum(result$train_pred == train.testLabels)/9912
train_pred <- knn(train = train.training, test = train.test, cl = train.trainLabels, k=7)
train_pred <- as.data.frame(train_pred)
#compare the results with the truth
result <- cbind(train_pred, train.testLabels)
sum(result$train_pred == train.testLabels)/9912
train_pred <- knn(train = train.training, test = train.test, cl = train.trainLabels, k=11)
train_pred <- as.data.frame(train_pred)
#compare the results with the truth
result <- cbind(train_pred, train.testLabels)
sum(result$train_pred == train.testLabels)/9912
train_pred <- knn(train = train.training, test = train.test, cl = train.trainLabels, k=15)
train_pred <- as.data.frame(train_pred)
#compare the results with the truth
result <- cbind(train_pred, train.testLabels)
sum(result$train_pred == train.testLabels)/9912
train_pred <- knn(train = train.training, test = train.test, cl = train.trainLabels, k=20)
train_pred <- as.data.frame(train_pred)
#compare the results with the truth
result <- cbind(train_pred, train.testLabels)
sum(result$train_pred == train.testLabels)/9912
test <- read.csv("../DATA/news_popularity_test.csv", sep = ",")
View(train)
real_train_pred <- knn(train=train[,3:61], test = test[,3:61], cl = train[,62], k=20)
real_train_pred <- as.data.frame(real_train_pred)
submit <- cbind(c(30000:39644), real_train_pred)
submit <- cbind(c(30001:39644), real_train_pred)
View(submit)
names(submit) <- c("id", "popularity")
View(submit)
?write.csv
write.csv(submit, file = "submit1.csv")
write.csv(submit, file = "submit1.csv")
?write.csv
write.csv(submit, file = "submit1.csv", quote = FALSE, row.names = FALSE)
library(class)
train <- read.csv("../DATA/news_popularity_training.csv", sep = ",")
test <- read.csv("../DATA/news_popularity_test.csv", sep = ",")
#tables
table(train$popularity)
round(prop.table(table(train$popularity)) * 100, digits = 1)
#normalize
#normalize <- function(x) {
# num <- x - min(x)
#denom <- max(x) - min(x)
#return (num/denom)
#}
train_norm <- as.data.frame(lapply(train[4:61], normalize))
train_norm <- cbind(train[1:3],train_norm,train$popularity)
?sample
#Sample
ind <- sample(2, nrow(train), replace=TRUE, prob=c(0.67, 0.33))
train.training <- train[ind==1, 3:61]
train.test <- train[ind==2, 3:61]
train.trainLabels <- train[ind==1, 62]
train.testLabels <- train[ind==2, 62]
c(1:5)
#kkn
train_pred <- knn(train = train.training, test = train.test, cl = train.trainLabels, k=20)
train_pred <- as.data.frame(train_pred)
#compare the results with the truth
result <- cbind(train_pred, train.testLabels)
#rate of correctness
sum(result$train_pred == train.testLabels)/9912
train_norm <- as.data.frame(lapply(train[4:61], normalize))
train_norm <- cbind(train[1:3],train_norm,train$popularity)
#normalize
normalize <- function(x) {
num <- x - min(x)
denom <- max(x) - min(x)
return (num/denom)
}
train_norm <- as.data.frame(lapply(train[4:61], normalize))
train_norm <- cbind(train[1:3],train_norm,train$popularity)
#normalized version
ind <- sample(2, nrow(train_norm), replace=TRUE, prob=c(0.67, 0.33))
train.training <- train_norm[ind==1, 3:61]
train.test <- train_norm[ind==2, 3:61]
train.trainLabels <- train_norm[ind==1, 62]
train.testLabels <- train_norm[ind==2, 62]
#kkn
train_pred <- knn(train = train.training, test = train.test, cl = train.trainLabels, k=20)
train_pred <- as.data.frame(train_pred)
#compare the results with the truth
result <- cbind(train_pred, train.testLabels)
#rate of correctness
sum(result$train_pred == train.testLabels)/9912
sum(result$train_pred == train.testLabels)/9952
train_stand <- as.data.frame(lapply(train[4:61], standardize))
train_stand <- cbind(train[1:3],train_stand,train$popularity)
#standardized version
standardize <- function(x) {
num <- x - mean(x)
denom <-sd(x)
return (num/denom)
}
train_stand <- as.data.frame(lapply(train[4:61], standardize))
train_stand <- cbind(train[1:3],train_stand,train$popularity)
ind <- sample(2, nrow(train_stand), replace=TRUE, prob=c(0.67, 0.33))
train.training <- train_stand[ind==1, 3:61]
train.test <- train_stand[ind==2, 3:61]
train.trainLabels <- train_stand[ind==1, 62]
train.testLabels <- train_stand[ind==2, 62]
ind <- sample(2, nrow(train_stand), replace=TRUE, prob=c(0.67, 0.33))
train.training <- train_stand[ind==1, 3:61]
train.test <- train_stand[ind==2, 3:61]
train.trainLabels <- train_stand[ind==1, 62]
train.testLabels <- train_stand[ind==2, 62]
#kkn
train_pred <- knn(train = train.training, test = train.test, cl = train.trainLabels, k=20)
train_pred <- as.data.frame(train_pred)
#compare the results with the truth
result <- cbind(train_pred, train.testLabels)
#rate of correctness
sum(result$train_pred == train.testLabels)/9934
sum(result$train_pred == train.testLabels)/10008
library(class)
train <- read.csv("../DATA/news_popularity_training.csv", sep = ",")
test <- read.csv("../DATA/news_popularity_test.csv", sep = ",")
#tables
table(train$popularity)
round(prop.table(table(train$popularity)) * 100, digits = 1)
#normalized version
ind <- sample(2, nrow(train_norm), replace=TRUE, prob=c(0.67, 0.33))
train.training <- train_norm[ind==1, 3:61]
train.test <- train_norm[ind==2, 3:61]
train.trainLabels <- train_norm[ind==1, 62]
train.testLabels <- train_norm[ind==2, 62]
#kkn
train_pred <- knn(train = train.training, test = train.test, cl = train.trainLabels, k=20)
train_pred <- as.data.frame(train_pred)
#compare the results with the truth
result <- cbind(train_pred, train.testLabels)
#rate of correctness
sum(result$train_pred == train.testLabels)/9952
#standardized version
standardize <- function(x) {
num <- x - mean(x)
denom <-sd(x)
return (num/denom)
}
train_stand <- as.data.frame(lapply(train[4:61], standardize))
train_stand <- cbind(train[1:3],train_stand,train$popularity)
test_stand <- as.data.frame(lapply(test[4:61], standardize))
test_stand <- cbind(test[1:3],test_stand)
real_train_pred <- knn(train=train_stand[,3:61], test = test_stand[,3:61], cl = train[,62], k=20)
real_train_pred <- as.data.frame(real_train_pred)
#data frame to submit
submit <- cbind(c(30001:39644), real_train_pred)
names(submit) <- c("id", "popularity")
write.csv(submit, file = "submit2.csv", quote = FALSE, row.names = FALSE)
train <- read.csv("../DATA/news_popularity_training.csv", sep = ",")
test <- read.csv("../DATA/news_popularity_test.csv", sep = ",")
table(train$popularity)
round(prop.table(table(train$popularity)) * 100, digits = 1)
train <- read.csv("../DATA/news_popularity_training.csv", sep = ",")
test <- read.csv("../DATA/news_popularity_test.csv", sep = ",")
standardize <- function(x) {
num <- x - mean(x)
denom <-sd(x)
return (num/denom)
}
View(train)
View(test)
count(which(test[,4] <=10))
library(plyr)
count(which(test[,4] <=10))
count(test[,4] <=10)
?count
