install.packages("devtools")
install.packages("dplyr")
install.packages("Rmarkdown")
install.packages("rmarkdown")
?sample
meansC1 <- rmvnorm(10, mean = c(1,0), sigma = diag(2))
install.packages("rmvtnorm")
install.packages("rmvtnorm", dependencies=TRUE)
setRepositories()
install.packages("mvtnorm", dependencies = TRUE)
meansC1 <- rmvnorm(10, mean = c(1,0), sigma = diag(2))
library(mvtnorm)
meansC1 <- rmvnorm(10, mean = c(1,0), sigma = diag(2))
meansC2 <- rmvnorm(10, mean = c(0,1), sigma = diag(2))
# for each observation we first randomly select one Gaussian and then
# generate a point according to the parameters of that Gaussian
mixtureProb, replace = TRUE)
whichGaussianC2 <- sample(nrow(meansC2), noObs[2],
mixtureProb, replace = TRUE)
mixtureProb, replace = TRUE)
rep(1/10, 10), replace = TRUE)
prob = rep(1/10, 10), replace = TRUE)
meansC1 <- rmvnorm(10, mean = c(1,0), sigma = diag(2))
meansC2 <- rmvnorm(10, mean = c(0,1), sigma = diag(2))
# for each observation we first randomly select one Gaussian and then
# generate a point according to the parameters of that Gaussian
whichGaussianC1 <- sample(nrow(meansC1), 100,
prob = rep(1/10, 10), replace = TRUE)
whichGaussianC2 <- sample(nrow(meansC2), 100,
mixtureProb, replace = TRUE)
prob = rep(1/10, 10), replace = TRUE)
prob = rep(1/10, 10), replace = TRUE)
prob = rep(1/10, 10), replace = TRUE)
whichGaussianC2 <- sample(nrow(meansC2), 100,
prob = rep(1/10, 10), replace = TRUE)
whichGaussianC1
sigma = diag(2)/5)) %>% t()
drawsC2 <- whichGaussianC2 %>%
sapply(function(x) rmvnorm(1, mean = meansC2[x,],
sigma = diag(2)/5)) %>% t()
library(dplyr)
meansC1 <- rmvnorm(10, mean = c(1,0), sigma = diag(2))
meansC2 <- rmvnorm(10, mean = c(0,1), sigma = diag(2))
# for each observation we first randomly select one Gaussian and then
# generate a point according to the parameters of that Gaussian
whichGaussianC1 <- sample(nrow(meansC1), 100,
prob = rep(1/10, 10), replace = TRUE)
whichGaussianC2 <- sample(nrow(meansC2), 100,
prob = rep(1/10, 10), replace = TRUE)
# now drawing samples from selected bivariate Gaussians
drawsC1 <- whichGaussianC1 %>%
sapply(function(x) rmvnorm(1, mean = meansC1[x,],
sigma = diag(2)/5)) %>% t()
drawsC2 <- whichGaussianC2 %>%
sapply(function(x) rmvnorm(1, mean = meansC2[x,],
sigma = diag(2)/5)) %>% t()
drawsC1
whichGaussianC2
meansC1
library(mvtnorm)
library(dplyr)
# 2 class 2 dimensional mixture of Gaussians
genGaussMix <- function(noObs = c(100, 100),
noGaussians = 10,
mixtureProb = rep(1/noGaussians, noGaussians),
seed = 2222) {
set.seed(seed)
# producing means of our bivariate Gaussians
meansC1 <- rmvnorm(noGaussians, mean = c(1,0), sigma = diag(2))
meansC2 <- rmvnorm(noGaussians, mean = c(0,1), sigma = diag(2))
# for each observation we first randomly select one Gaussian and then
# generate a point according to the parameters of that Gaussian
whichGaussianC1 <- sample(nrow(meansC1), noObs[1],
mixtureProb, replace = TRUE)
whichGaussianC2 <- sample(nrow(meansC2), noObs[2],
mixtureProb, replace = TRUE)
# now drawing samples from selected bivariate Gaussians
drawsC1 <- whichGaussianC1 %>%
sapply(function(x) rmvnorm(1, mean = meansC1[x,],
sigma = diag(2)/5)) %>% t()
drawsC2 <- whichGaussianC2 %>%
sapply(function(x) rmvnorm(1, mean = meansC2[x,],
sigma = diag(2)/5)) %>% t()
# combining and labeling
dataset <- data.frame(rbind(drawsC1, drawsC2),
label = c(rep("C1", noObs[1]), rep("C2", noObs[2])),
y = c(rep(0, noObs[1]), rep(1, noObs[2])),
stringsAsFactors = FALSE)
return(dataset)
}
# plotting function
plot2dClasses <- function(dataset) {
ggplot(data = dataset,
aes(x = X1, y = X2, colour = factor(y))) +
geom_point(size = 2, shape = 4) +
xlab("X1") +
ylab("X2") +
theme_bw() +
theme(text=element_text(family="Helvetica")) +
scale_color_manual("Class",
values = c("0" = "blue", "1" = "red"))
}
# generate some data
dataset <- genGaussMix()
str(dataset)
# plotting function
plot2dClasses <- function(dataset) {
ggplot(data = dataset,
aes(x = X1, y = X2, colour = factor(y))) +
geom_point(size = 2, shape = 4) +
xlab("X1") +
ylab("X2") +
theme_bw() +
theme(text=element_text(family="Helvetica")) +
scale_color_manual("Class",
values = c("0" = "blue", "1" = "red"))
}
plot2dClasses
dataset <- genGaussMix()
str(dataset)
plot2dClasses(dataset)
install.packages("ggplot2")
library(ggplot2)
plot2dClasses(dataset)
genGaussMix <- function(noObs = c(100, 100),
noGaussians = 10,
mixtureProb = rep(1/noGaussians, noGaussians),
seed = 1111) {
set.seed(seed)
# producing means of our bivariate Gaussians
meansC1 <- rmvnorm(noGaussians, mean = c(1,0), sigma = diag(2))
meansC2 <- rmvnorm(noGaussians, mean = c(0,1), sigma = diag(2))
# for each observation we first randomly select one Gaussian and then
# generate a point according to the parameters of that Gaussian
whichGaussianC1 <- sample(nrow(meansC1), noObs[1],
mixtureProb, replace = TRUE)
whichGaussianC2 <- sample(nrow(meansC2), noObs[2],
mixtureProb, replace = TRUE)
# now drawing samples from selected bivariate Gaussians
drawsC1 <- whichGaussianC1 %>%
sapply(function(x) rmvnorm(1, mean = meansC1[x,],
sigma = diag(2)/5)) %>% t()
drawsC2 <- whichGaussianC2 %>%
sapply(function(x) rmvnorm(1, mean = meansC2[x,],
sigma = diag(2)/5)) %>% t()
# combining and labeling
dataset <- data.frame(rbind(drawsC1, drawsC2),
label = c(rep("C1", noObs[1]), rep("C2", noObs[2])),
y = c(rep(0, noObs[1]), rep(1, noObs[2])),
stringsAsFactors = FALSE)
return(dataset)
}
# plotting function
plot2dClasses <- function(dataset) {
ggplot(data = dataset,
aes(x = X1, y = X2, colour = factor(y))) +
geom_point(size = 2, shape = 4) +
xlab("X1") +
ylab("X2") +
theme_bw() +
theme(text=element_text(family="Helvetica")) +
scale_color_manual("Class",
values = c("0" = "blue", "1" = "red"))
}
# generate some data
dataset <- genGaussMix()
str(dataset)
plot2dClasses(dataset)
k <- 1  # odd number
p <- 2  # Manhattan (1), Euclidean (2) or Chebyshev (Inf)
# Compute the distance between each point and all others
# according to the similarity measure
noObs <- nrow(dataset)
distMatrix <- matrix(NA, noObs, noObs)
View(dataset)
features <- as.matrix(dataset[ ,1:2])
View(features)
View(features)
for (obs in 1:noObs) {
# getting the probe for the current observation
probe <- features[obs,]
probeExpanded <- matrix(probe, nrow = noObs, ncol = 2, byrow = TRUE)
# computing distances between the probe and exemplars in the memory
if (p %in% c(1,2)) {
distMatrix[obs, ] <- ( rowSums((abs(features -
probeExpanded))^p) )^(1/p)
} else if (p==Inf) {
distMatrix[obs, ] <- apply(abs(features - probeExpanded), 1, max)
}
}
View(probeExpanded)
neighbors <- apply(distMatrix, 2, order) %>% t()
# first entry for the first point should be the closest neighbor
neighbors[1,1] == 1
## [1] TRUE
# the most frequent class in the k nearest neighbors
prob <- rep(NA, noObs)
for (obs in 1:noObs) {
prob[obs] <- mean(dataset[neighbors[obs, 1:k], "y"])
}
# predicted label
predictedClasses <- ifelse(prob > 0.5, 1, 0)
table(predictedClasses, dataset[,"y"])
##
## predictedClasses   0   1
##                0 100   0
##                1   0 100
mean(predictedClasses == dataset[,"y"])
?apply
View(neighbors)
neighbors <- apply(distMatrix, 1, order) %>% t()
View(neighbors)
all(diag(distMatrix) == 0)
View(distMatrix)
distMatrix
View(neighbors)
View(probeExpanded)
View(neighbors)
neighbors[1,1] == 1
View(neighbors)
neigbors[2,1]
neighbors[2,1]
View(neighbors)
View(dataset)
View(dataset)
table(predictedClasses, dataset[,"y"])
mean(predictedClasses == dataset[,"y"])
install.packages("assert_that")
install.packages("assertthat")
install.packages("assertthat")
install.packages("assertthat")
kNN <- function(X, y, memory = NULL,
k = 1, p = 2, type="train") {
# test the inputs
library(assertthat)
not_empty(X); not_empty(y);
if (type == "train") {
assert_that(nrow(X) == length(y))
}
is.string(type); assert_that(type %in% c("train", "predict"))
is.count(k);
assert_that(p %in% c(1, 2, Inf))
if (type == "predict") {
assert_that(not_empty(memory) &
ncol(memory) == ncol(X) &
nrow(memory) == length(y))
}
# Compute the distance between each point and all others
noObs <- nrow(X)
# if we are making predictions on the test set based on the memory,
# we compute distances between each test observation and observations
# in our memory
if (type == "train") {
distMatrix <- matrix(NA, noObs, noObs)
for (obs in 1:noObs) {
# getting the probe for the current observation
probe <- as.numeric(X[obs,])
probeExpanded <- matrix(probe, nrow = noObs, ncol = 2,
byrow = TRUE)
# computing distances between the probe and exemplars in the
# training X
if (p %in% c(1,2)) {
distMatrix[obs, ] <- (rowSums((abs(X -
probeExpanded))^p) )^(1/p)
} else if (p==Inf) {
distMatrix[obs, ] <- apply(abs(X - probeExpanded), 1, max)
}
}
} else if (type == "predict") {
noMemory <- nrow(memory)
distMatrix <- matrix(NA, noObs, noMemory)
for (obs in 1:noObs) {
# getting the probe for the current observation
probe <- as.numeric(X[obs,])
probeExpanded <- matrix(probe, nrow = noMemory, ncol = 2,
byrow = TRUE)
# computing distances between the probe and exemplars in the memory
if (p %in% c(1,2)) {
distMatrix[obs, ] <- (rowSums((abs(memory -
probeExpanded))^p) )^(1/p)
} else if (p==Inf) {
distMatrix[obs, ] <- apply(abs(memory - probeExpanded), 1, max)
}
}
}
# Sort the distances in increasing numerical order and pick the first
# k elements
neighbors <- apply(distMatrix, 1, order)
# Compute and return the most frequent class in the k nearest neighbors
prob <- rep(NA, noObs)
for (obs in 1:noObs) {
prob[obs] <- mean(y[neighbors[1:k, obs]])
}
# predicted label
predictedClasses <- ifelse(prob > 0.5, 1, 0)
# examine the performance, available only if training
if (type == "train") {
errorCount <- table(predictedClasses, y)
accuracy <- mean(predictedClasses == y)
} else if (type == "predict") {
errorCount <- NA
accuracy <- NA
}
# return the results
return(list(predictedClasses = predictedClasses,
prob = prob,
accuracy = accuracy,
errorCount = errorCount))
}
?apply
?new.env
install.packages("class")
Sys.getenv("SPARK_HOME")
sparkR.init()
Sys.getenv("SPARK_HOME")
library(sparkR)
Sys.getenv("SPARK_HOME")
Sys.getenv("SPARK_HOME")
Sys.getenv("PATH")
Sys.getenv("SPARK_HOME")
Sys.setenv("SPARK_HOME=/Users/guglielmo/Documents/spark-1.6.0-bin-hadoop2.6")
?Sys.setenv
Sys.setenv(SPARK_HOME="/Users/guglielmo/Documents/spark-1.6.0-bin-hadoop2.6")
Sys.getenv("SPARK_HOME")
library(sparkR)
library(SparkR)
.libPaths(c(file.path(Sys.getenv(“SPARK_HOME”), “R”, “lib”), .libPaths()))
.libPaths(c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib"), .libPaths()))
library(SparkR)
if(!require("class")) install.packages("class"); library(class)
if(!require("HotDeckImputation")) install.packages("HotDeckImputation"); library(HotDeckImputation)
#get the data
train <- read.csv("../DATA/news_popularity_training.csv", sep = ",")
test <- read.csv("../DATA/news_popularity_test.csv", sep = ",")
test$popularity <- NA
dataset <- rbind(train,test)
features <- dataset[,-which(colnames(dataset) %in% c("popularity"))]
getwd()
setwd("~/Desktop/BGSE/Kaggle_Competition/Code")
if(!require("class")) install.packages("class"); library(class)
if(!require("HotDeckImputation")) install.packages("HotDeckImputation"); library(HotDeckImputation)
#get the data
train <- read.csv("../DATA/news_popularity_training.csv", sep = ",")
test <- read.csv("../DATA/news_popularity_test.csv", sep = ",")
test$popularity <- NA
dataset <- rbind(train,test)
features <- dataset[,-which(colnames(dataset) %in% c("popularity"))]
three.cat <- function(x){
for(i in 1:length(x)){
if(x[i] > 2) x[i] <- 2
}
return(x)
}
features$num_imgs <- three.cat(features$num_imgs)
features$num_videos <- three.cat(features$num_videos)
#Remove non-sense or redundant features:
# Remove the constant column
features$n_non_stop_words <- NULL
# Remove the rate negative_words
features$rate_negative_words <- NULL
# # Remove the ukrain outlier
# features <- features[-which(features$n_unique_tokens > 1),]
# labels <- labels[-which(dataset$n_unique_tokens > 1)]
# Recode the missing values
features$n_unique_tokens[features$n_tokens_content == 0] <- NA
features$n_non_stop_unique_tokens[features$n_tokens_content == 0] <- NA
features$num_hrefs[features$n_tokens_content == 0] <- NA
features$num_self_hrefs[features$n_tokens_content == 0] <- NA
features$average_token_length[features$n_tokens_content == 0] <- NA
features$n_tokens_content[features$n_tokens_content == 0] <- NA
# Recode the missing values
features$global_sentiment_polarity[features$global_subjectivity == 0] <- NA
features$global_rate_positive_words[features$global_subjectivity == 0] <- NA
features$global_rate_negative_words[features$global_subjectivity == 0] <- NA
features$rate_positive_words[features$global_subjectivity == 0] <- NA
features$avg_positive_polarity[features$global_subjectivity == 0] <- NA
features$avg_negative_polarity[features$global_subjectivity == 0] <- NA
features$min_positive_polarity[features$global_subjectivity == 0] <- NA
features$min_negative_polarity[features$global_subjectivity == 0] <- NA
features$max_positive_polarity[features$global_subjectivity == 0] <- NA
features$max_negative_polarity[features$global_subjectivity == 0] <- NA
features$global_subjectivity[features$global_subjectivity == 0] <- NA
# Hot deck Imputation
View(features)
imp.features <- impute.NN_HD(DATA=features[,-c(1:3)],distance="eukl")
imp.features <- data.frame(url = features$url, imp.features)
obtain.date <- function(dataset){
dates = c("2013-01-01","2013-01-21","2013-02-18" , "2013-05-27", "2013-07-04",
"2013-09-02", "2013-10-14", "2013-11-11", "2013-11-28",
"2013-12-25", "2014-01-01", "2014-01-20", "2014-02-17", "2014-05-26",
"2014-07-04", "2014-09-01", "2014-10-13", "2014-11-11", "2014-11-27" ,
"2014-12-25")
myholidays  <- as.Date(dates,format ="%Y-%m-%d")
year <- as.numeric(substring(dataset$url, 21,24))
month <- as.numeric(substring(dataset$url, 26,27))
day <- as.numeric(substring(dataset$url, 29,30))
date <- paste(year,month,day, sep = "-")
date <- as.Date(date)
is_holiday <- rep(0,length(year))
is_holiday[which(date %in% myholidays)] <- 1
a <- data.frame(year = year,month = month, day = day, date = as.character(date), is_holiday = as.numeric(is_holiday))
return(a)
}
#obtain dates for training set
obtained.info <- obtain.date(imp.features)
imp.features <- data.frame(imp.features, year = obtained.info$year,month = obtained.info$month,
is_holiday = obtained.info$is_holiday)
imp.features$url <- NULL
features_clean = imp.features
View(features_clean)
train.clean <- data.frame(features_clean,popularity = as.numeric(dataset$popularity))[1:30000,]
#Split into popular and not popular
train.clean$dummy[train.clean$popularity < 3] <- 0
train.clean$dummy[train.clean$popularity > 2] <- 1
fisher.rank <- function(feature,label){
num <- (mean(feature[label == 0]) - mean(feature[label == 1]))^2
denom <- var(feature[label == 0]) + var(feature[label == 1])
rank <- round(num/denom,4)
return(rank)
}
X = train.clean[,-which(colnames(train.clean) %in% c("popularity","dummy"))]
fisher.score = apply(X,2,function(x)fisher.rank(x,train.clean$dummy))
names(fisher.score) = colnames(X)
top.ranks = fisher.score[order(fisher.score,decreasing = T)]
top.vars = names(top.ranks[1:20])
top.vars
train.clean = train.clean[,which(colnames(train.clean) %in% c(top.vars,"popularity"))]
test.clean = features_clean[30001:39644,which(colnames(train.clean) %in% top.vars)]
real_train_pred <- knn(train=train.clean[,-21], test = test.clean, cl = train.clean$popularity, k=20)
real_train_pred <- as.data.frame(real_train_pred)
final <- read.csv("/Users/guglielmo/Desktop/final_competition/final.csv", header = TRUE, sep = ",")
sum(real_train_pred == final$popularity )/9644
top.vars
top.vars = names(top.ranks[1:40])
train.clean = train.clean[,which(colnames(train.clean) %in% c(top.vars,"popularity"))]
test.clean = features_clean[30001:39644,which(colnames(train.clean) %in% top.vars)]
real_train_pred <- knn(train=train.clean[,-21], test = test.clean, cl = train.clean$popularity, k=20)
real_train_pred <- as.data.frame(real_train_pred)
final <- read.csv("/Users/guglielmo/Desktop/final_competition/final.csv", header = TRUE, sep = ",")
sum(real_train_pred == final$popularity )/9644
top.ranks = fisher.score[order(fisher.score,decreasing = T)]
top.vars = names(top.ranks[1:40])
train.clean = train.clean[,which(colnames(train.clean) %in% c(top.vars,"popularity"))]
train.clean <- data.frame(features_clean,popularity = as.numeric(dataset$popularity))[1:30000,]
#Split into popular and not popular
train.clean$dummy[train.clean$popularity < 3] <- 0
train.clean$dummy[train.clean$popularity > 2] <- 1
fisher.rank <- function(feature,label){
num <- (mean(feature[label == 0]) - mean(feature[label == 1]))^2
denom <- var(feature[label == 0]) + var(feature[label == 1])
rank <- round(num/denom,4)
return(rank)
}
X = train.clean[,-which(colnames(train.clean) %in% c("popularity","dummy"))]
fisher.score = apply(X,2,function(x)fisher.rank(x,train.clean$dummy))
names(fisher.score) = colnames(X)
top.ranks = fisher.score[order(fisher.score,decreasing = T)]
top.vars = names(top.ranks[1:40])
train.clean = train.clean[,which(colnames(train.clean) %in% c(top.vars,"popularity"))]
test.clean = features_clean[30001:39644,which(colnames(train.clean) %in% top.vars)]
real_train_pred <- knn(train=train.clean[,-41], test = test.clean, cl = train.clean$popularity, k=20)
real_train_pred <- as.data.frame(real_train_pred)
final <- read.csv("/Users/guglielmo/Desktop/final_competition/final.csv", header = TRUE, sep = ",")
sum(real_train_pred == final$popularity )/9644
