denom <-sd(x)
return (num/denom)
}
#categorical standardization
cat_stand <- function(x){
num <- x - 1/2
denom <- length(unique(x))
return(num/denom)
}
features.con <- apply(imp.features[,c(1:10,17:28,37:56)],2,standardize)
features.dis <- apply(imp.features[,-c(1:10,17:28,37:56)],2,cat_stand)
head(features.dis)
if(!require("class")) install.packages("class"); library(class)
if(!require("HotDeckImputation")) install.packages("HotDeckImputation"); library(HotDeckImputation)
#get the data
train <- read.csv("../DATA/news_popularity_training.csv", sep = ",")
test <- read.csv("../DATA/news_popularity_test.csv", sep = ",")
test$popularity <- NA
dataset <- rbind(train,test)
features <- dataset[,c(2,4:61)]
labels <- as.factor(dataset$popularity)
###############################################################
##DATA MANIPULATION & CLEANING
#transform number of images & videos into 3-categorical variables (0, 1 or more than 1)
three.cat <- function(x){
for(i in 1:length(x)){
if(x[i] > 2) x[i] <- 2
}
return(x)
}
features$num_imgs <- three.cat(features$num_imgs)
features$num_videos <- three.cat(features$num_videos)
#Remove non-sense or redundant features:
# Remove the constant column
features$n_non_stop_words <- NULL
# Remove the rate negative_words
features$rate_negative_words <- NULL
# Remove the ukrain outlier
features <- features[-which(features$n_unique_tokens > 1),]
# Recode the missing values
features$n_unique_tokens[features$n_tokens_content == 0] <- NA
features$n_non_stop_unique_tokens[features$n_tokens_content == 0] <- NA
features$num_hrefs[features$n_tokens_content == 0] <- NA
features$num_self_hrefs[features$n_tokens_content == 0] <- NA
features$average_token_length[features$n_tokens_content == 0] <- NA
features$n_tokens_content[features$n_tokens_content == 0] <- NA
# Recode the missing values
features$global_sentiment_polarity[features$global_subjectivity == 0] <- NA
features$global_rate_positive_words[features$global_subjectivity == 0] <- NA
features$global_rate_negative_words[features$global_subjectivity == 0] <- NA
features$rate_positive_words[features$global_subjectivity == 0] <- NA
features$avg_positive_polarity[features$global_subjectivity == 0] <- NA
features$avg_negative_polarity[features$global_subjectivity == 0] <- NA
features$min_positive_polarity[features$global_subjectivity == 0] <- NA
features$min_negative_polarity[features$global_subjectivity == 0] <- NA
features$max_positive_polarity[features$global_subjectivity == 0] <- NA
features$max_negative_polarity[features$global_subjectivity == 0] <- NA
features$global_subjectivity[features$global_subjectivity == 0] <- NA
# Hot deck Imputation
imp.features <- impute.NN_HD(DATA=features[,-1],distance="eukl")
imp.features <- data.frame(url = features$url, imp.features)
#obtain date & holiday variables
#New years, Martin Luther, Washington's Birthday, Memorial day, Independence, labor, Columbus
#Veterans, Thanksgiving, Christmas
obtain.date <- function(dataset){
dates = c("2013-01-01","2013-01-21","2013-02-18" , "2013-05-27", "2013-07-04",
"2013-09-02", "2013-10-14", "2013-11-11", "2013-11-28",
"2013-12-25", "2014-01-01", "2014-01-20", "2014-02-17", "2014-05-26",
"2014-07-04", "2014-09-01", "2014-10-13", "2014-11-11", "2014-11-27" ,
"2014-12-25")
myholidays  <- as.Date(dates,format ="%Y-%m-%d")
year <- as.numeric(substring(dataset$url, 21,24))
month <- as.numeric(substring(dataset$url, 26,27))
day <- as.numeric(substring(dataset$url, 29,30))
date <- paste(year,month,day, sep = "-")
date <- as.Date(date)
is_holiday <- rep(0,length(year))
is_holiday[which(date %in% myholidays)] <- 1
a <- as.data.frame(year = year,month = month, day = day, date = as.character(date), is_holiday = as.numeric(is_holiday))
return(a)
}
#obtain dates for training set
obtained.info <- obtain.date(imp.features)
#append the new created features
imp.features <- data.frame(imp.features, year = obtained.info$year,month = obtained.info$month,
is_holiday = obtained.info$is_holiday)
imp.features$url <- NULL
######Standardization of the features
#standardize the continuous features
standardize <- function(x) {
num <- x - mean(x)
denom <-sd(x)
return (num/denom)
}
#categorical standardization
cat_stand <- function(x){
num <- x - 1/2
denom <- length(unique(x))
return(num/denom)
}
if(!require("class")) install.packages("class"); library(class)
if(!require("HotDeckImputation")) install.packages("HotDeckImputation"); library(HotDeckImputation)
#get the data
train <- read.csv("../DATA/news_popularity_training.csv", sep = ",")
test <- read.csv("../DATA/news_popularity_test.csv", sep = ",")
test$popularity <- NA
dataset <- rbind(train,test)
features <- dataset[,c(2,4:61)]
labels <- as.factor(dataset$popularity)
###############################################################
##DATA MANIPULATION & CLEANING
#transform number of images & videos into 3-categorical variables (0, 1 or more than 1)
three.cat <- function(x){
for(i in 1:length(x)){
if(x[i] > 2) x[i] <- 2
}
return(x)
}
features$num_imgs <- three.cat(features$num_imgs)
features$num_videos <- three.cat(features$num_videos)
#Remove non-sense or redundant features:
# Remove the constant column
features$n_non_stop_words <- NULL
# Remove the rate negative_words
features$rate_negative_words <- NULL
# Remove the ukrain outlier
features <- features[-which(features$n_unique_tokens > 1),]
# Recode the missing values
features$n_unique_tokens[features$n_tokens_content == 0] <- NA
features$n_non_stop_unique_tokens[features$n_tokens_content == 0] <- NA
features$num_hrefs[features$n_tokens_content == 0] <- NA
features$num_self_hrefs[features$n_tokens_content == 0] <- NA
features$average_token_length[features$n_tokens_content == 0] <- NA
features$n_tokens_content[features$n_tokens_content == 0] <- NA
# Recode the missing values
features$global_sentiment_polarity[features$global_subjectivity == 0] <- NA
features$global_rate_positive_words[features$global_subjectivity == 0] <- NA
features$global_rate_negative_words[features$global_subjectivity == 0] <- NA
features$rate_positive_words[features$global_subjectivity == 0] <- NA
features$avg_positive_polarity[features$global_subjectivity == 0] <- NA
features$avg_negative_polarity[features$global_subjectivity == 0] <- NA
features$min_positive_polarity[features$global_subjectivity == 0] <- NA
features$min_negative_polarity[features$global_subjectivity == 0] <- NA
features$max_positive_polarity[features$global_subjectivity == 0] <- NA
features$max_negative_polarity[features$global_subjectivity == 0] <- NA
features$global_subjectivity[features$global_subjectivity == 0] <- NA
# Hot deck Imputation
imp.features <- impute.NN_HD(DATA=features[,-1],distance="eukl")
imp.features <- data.frame(url = features$url, imp.features)
#obtain date & holiday variables
#New years, Martin Luther, Washington's Birthday, Memorial day, Independence, labor, Columbus
#Veterans, Thanksgiving, Christmas
obtain.date <- function(dataset){
dates = c("2013-01-01","2013-01-21","2013-02-18" , "2013-05-27", "2013-07-04",
"2013-09-02", "2013-10-14", "2013-11-11", "2013-11-28",
"2013-12-25", "2014-01-01", "2014-01-20", "2014-02-17", "2014-05-26",
"2014-07-04", "2014-09-01", "2014-10-13", "2014-11-11", "2014-11-27" ,
"2014-12-25")
myholidays  <- as.Date(dates,format ="%Y-%m-%d")
year <- as.numeric(substring(dataset$url, 21,24))
month <- as.numeric(substring(dataset$url, 26,27))
day <- as.numeric(substring(dataset$url, 29,30))
date <- paste(year,month,day, sep = "-")
date <- as.Date(date)
is_holiday <- rep(0,length(year))
is_holiday[which(date %in% myholidays)] <- 1
a <- data.frame(year = year,month = month, day = day, date = as.character(date), is_holiday = as.numeric(is_holiday))
return(a)
}
#obtain dates for training set
obtained.info <- obtain.date(imp.features)
#append the new created features
imp.features <- data.frame(imp.features, year = obtained.info$year,month = obtained.info$month,
is_holiday = obtained.info$is_holiday)
imp.features$url <- NULL
######Standardization of the features
#standardize the continuous features
standardize <- function(x) {
num <- x - mean(x)
denom <-sd(x)
return (num/denom)
}
#categorical standardization
cat_stand <- function(x){
num <- x - 1/2
denom <- length(unique(x))
return(num/denom)
}
View(obtained.info)
features.con <- apply(imp.features[,c(1:10,17:28,37:56)],2,standardize)
features.dis <- apply(imp.features[,-c(1:10,17:28,37:56)],2,cat_stand)
head(features.dis)
if(!require("class")) install.packages("class"); library(class)
if(!require("HotDeckImputation")) install.packages("HotDeckImputation"); library(HotDeckImputation)
#get the data
train <- read.csv("../DATA/news_popularity_training.csv", sep = ",")
test <- read.csv("../DATA/news_popularity_test.csv", sep = ",")
test$popularity <- NA
dataset <- rbind(train,test)
features <- dataset[,c(2,4:61)]
labels <- as.factor(dataset$popularity)
###############################################################
##DATA MANIPULATION & CLEANING
#transform number of images & videos into 3-categorical variables (0, 1 or more than 1)
three.cat <- function(x){
for(i in 1:length(x)){
if(x[i] > 2) x[i] <- 2
}
return(x)
}
features$num_imgs <- three.cat(features$num_imgs)
features$num_videos <- three.cat(features$num_videos)
#Remove non-sense or redundant features:
# Remove the constant column
features$n_non_stop_words <- NULL
# Remove the rate negative_words
features$rate_negative_words <- NULL
# Remove the ukrain outlier
features <- features[-which(features$n_unique_tokens > 1),]
# Recode the missing values
features$n_unique_tokens[features$n_tokens_content == 0] <- NA
features$n_non_stop_unique_tokens[features$n_tokens_content == 0] <- NA
features$num_hrefs[features$n_tokens_content == 0] <- NA
features$num_self_hrefs[features$n_tokens_content == 0] <- NA
features$average_token_length[features$n_tokens_content == 0] <- NA
features$n_tokens_content[features$n_tokens_content == 0] <- NA
# Recode the missing values
features$global_sentiment_polarity[features$global_subjectivity == 0] <- NA
features$global_rate_positive_words[features$global_subjectivity == 0] <- NA
features$global_rate_negative_words[features$global_subjectivity == 0] <- NA
features$rate_positive_words[features$global_subjectivity == 0] <- NA
features$avg_positive_polarity[features$global_subjectivity == 0] <- NA
features$avg_negative_polarity[features$global_subjectivity == 0] <- NA
features$min_positive_polarity[features$global_subjectivity == 0] <- NA
features$min_negative_polarity[features$global_subjectivity == 0] <- NA
features$max_positive_polarity[features$global_subjectivity == 0] <- NA
features$max_negative_polarity[features$global_subjectivity == 0] <- NA
features$global_subjectivity[features$global_subjectivity == 0] <- NA
# Hot deck Imputation
imp.features <- impute.NN_HD(DATA=features[,-1],distance="eukl")
imp.features <- data.frame(url = features$url, imp.features)
#obtain date & holiday variables
#New years, Martin Luther, Washington's Birthday, Memorial day, Independence, labor, Columbus
#Veterans, Thanksgiving, Christmas
obtain.date <- function(dataset){
dates = c("2013-01-01","2013-01-21","2013-02-18" , "2013-05-27", "2013-07-04",
"2013-09-02", "2013-10-14", "2013-11-11", "2013-11-28",
"2013-12-25", "2014-01-01", "2014-01-20", "2014-02-17", "2014-05-26",
"2014-07-04", "2014-09-01", "2014-10-13", "2014-11-11", "2014-11-27" ,
"2014-12-25")
myholidays  <- as.Date(dates,format ="%Y-%m-%d")
year <- as.numeric(substring(dataset$url, 21,24))
month <- as.numeric(substring(dataset$url, 26,27))
day <- as.numeric(substring(dataset$url, 29,30))
date <- paste(year,month,day, sep = "-")
date <- as.Date(date)
is_holiday <- rep(0,length(year))
is_holiday[which(date %in% myholidays)] <- 1
a <- data.frame(year = year,month = month, day = day, date = as.character(date), is_holiday = as.numeric(is_holiday))
return(a)
}
#obtain dates for training set
obtained.info <- obtain.date(imp.features)
#append the new created features
imp.features <- data.frame(imp.features, year = obtained.info$year,month = obtained.info$month,
is_holiday = obtained.info$is_holiday)
imp.features$url <- NULL
######Standardization of the features
#standardize the continuous features
standardize <- function(x) {
num <- x - mean(x)
denom <-sd(x)
return (num/denom)
}
#categorical standardization
cat_stand <- function(x){
num <- x - min(x)
denom <- max(x)-min(x)
return(num/denom)
}
features.con <- apply(imp.features[,c(1:10,17:28,37:56)],2,standardize)
features.dis <- apply(imp.features[,-c(1:10,17:28,37:56)],2,cat_stand)
head(features.dis)
features_clean <- cbind(features.con, features.dis)
features_clean <- as.data.frame(cbind(features.con, features.dis))
View(features_clean)
features_clean <- data.frame(features.con, features.dis)
View(features_clean)
if(!require("class")) install.packages("class"); library(class)
if(!require("HotDeckImputation")) install.packages("HotDeckImputation"); library(HotDeckImputation)
#get the data
train <- read.csv("../DATA/news_popularity_training.csv", sep = ",")
test <- read.csv("../DATA/news_popularity_test.csv", sep = ",")
test$popularity <- NA
dataset <- rbind(train,test)
features <- dataset[,c(2,4:61)]
labels <- as.factor(dataset$popularity)
###############################################################
##DATA MANIPULATION & CLEANING
#transform number of images & videos into 3-categorical variables (0, 1 or more than 1)
three.cat <- function(x){
for(i in 1:length(x)){
if(x[i] > 2) x[i] <- 2
}
return(x)
}
features$num_imgs <- three.cat(features$num_imgs)
features$num_videos <- three.cat(features$num_videos)
#Remove non-sense or redundant features:
# Remove the constant column
features$n_non_stop_words <- NULL
# Remove the rate negative_words
features$rate_negative_words <- NULL
# Remove the ukrain outlier
features <- features[-which(features$n_unique_tokens > 1),]
labels <- labels[-which(features$n_unique_tokens > 1),]
if(!require("class")) install.packages("class"); library(class)
if(!require("HotDeckImputation")) install.packages("HotDeckImputation"); library(HotDeckImputation)
#get the data
train <- read.csv("../DATA/news_popularity_training.csv", sep = ",")
test <- read.csv("../DATA/news_popularity_test.csv", sep = ",")
test$popularity <- NA
dataset <- rbind(train,test)
features <- dataset[,c(2,4:61)]
labels <- as.factor(dataset$popularity)
###############################################################
##DATA MANIPULATION & CLEANING
#transform number of images & videos into 3-categorical variables (0, 1 or more than 1)
three.cat <- function(x){
for(i in 1:length(x)){
if(x[i] > 2) x[i] <- 2
}
return(x)
}
features$num_imgs <- three.cat(features$num_imgs)
features$num_videos <- three.cat(features$num_videos)
#Remove non-sense or redundant features:
# Remove the constant column
features$n_non_stop_words <- NULL
# Remove the rate negative_words
features$rate_negative_words <- NULL
# Remove the ukrain outlier
features <- features[-which(features$n_unique_tokens > 1),]
labels <- labels[-which(features$n_unique_tokens > 1)]
# Recode the missing values
features$n_unique_tokens[features$n_tokens_content == 0] <- NA
features$n_non_stop_unique_tokens[features$n_tokens_content == 0] <- NA
features$num_hrefs[features$n_tokens_content == 0] <- NA
features$num_self_hrefs[features$n_tokens_content == 0] <- NA
features$average_token_length[features$n_tokens_content == 0] <- NA
features$n_tokens_content[features$n_tokens_content == 0] <- NA
# Recode the missing values
features$global_sentiment_polarity[features$global_subjectivity == 0] <- NA
features$global_rate_positive_words[features$global_subjectivity == 0] <- NA
features$global_rate_negative_words[features$global_subjectivity == 0] <- NA
features$rate_positive_words[features$global_subjectivity == 0] <- NA
features$avg_positive_polarity[features$global_subjectivity == 0] <- NA
features$avg_negative_polarity[features$global_subjectivity == 0] <- NA
features$min_positive_polarity[features$global_subjectivity == 0] <- NA
features$min_negative_polarity[features$global_subjectivity == 0] <- NA
features$max_positive_polarity[features$global_subjectivity == 0] <- NA
features$max_negative_polarity[features$global_subjectivity == 0] <- NA
features$global_subjectivity[features$global_subjectivity == 0] <- NA
# Hot deck Imputation
imp.features <- impute.NN_HD(DATA=features[,-1],distance="eukl")
imp.features <- data.frame(url = features$url, imp.features)
#obtain date & holiday variables
#New years, Martin Luther, Washington's Birthday, Memorial day, Independence, labor, Columbus
#Veterans, Thanksgiving, Christmas
obtain.date <- function(dataset){
dates = c("2013-01-01","2013-01-21","2013-02-18" , "2013-05-27", "2013-07-04",
"2013-09-02", "2013-10-14", "2013-11-11", "2013-11-28",
"2013-12-25", "2014-01-01", "2014-01-20", "2014-02-17", "2014-05-26",
"2014-07-04", "2014-09-01", "2014-10-13", "2014-11-11", "2014-11-27" ,
"2014-12-25")
myholidays  <- as.Date(dates,format ="%Y-%m-%d")
year <- as.numeric(substring(dataset$url, 21,24))
month <- as.numeric(substring(dataset$url, 26,27))
day <- as.numeric(substring(dataset$url, 29,30))
date <- paste(year,month,day, sep = "-")
date <- as.Date(date)
is_holiday <- rep(0,length(year))
is_holiday[which(date %in% myholidays)] <- 1
a <- data.frame(year = year,month = month, day = day, date = as.character(date), is_holiday = as.numeric(is_holiday))
return(a)
}
#obtain dates for training set
obtained.info <- obtain.date(imp.features)
#append the new created features
imp.features <- data.frame(imp.features, year = obtained.info$year,month = obtained.info$month,
is_holiday = obtained.info$is_holiday)
imp.features$url <- NULL
######Standardization of the features
#standardize the continuous features
standardize <- function(x) {
num <- x - mean(x)
denom <-sd(x)
return (num/denom)
}
#categorical standardization
cat_stand <- function(x){
num <- x - min(x)
denom <- max(x)-min(x)
return(num/denom)
}
#apply the changes (to be corrected)
features.con <- apply(imp.features[,c(1:10,17:28,37:56)],2,standardize)
features.dis <- apply(imp.features[,-c(1:10,17:28,37:56)],2,cat_stand)
features_clean <- data.frame(features.con, features.dis)
View(features_clean)
real_train_pred <- knn(train=features_clean[1:38999,], test = features_clean[39000:39643,], cl = labels, k=20)
real_train_pred <- knn(train=features_clean[1:29999,], test = features_clean[30000:39643,], cl = labels, k=20)
real_train_pred <- knn(train=features_clean[1:29999,], test = features_clean[30000:39643,], cl = labels[1:29999], k=20)
View(features_clean)
table(labels)
labels
labels <- as.factor(dataset$popularity)
labels <- labels[-which(features$n_unique_tokens > 1)]
labels
-which(features$n_unique_tokens > 1)
-which(dataset$n_unique_tokens > 1)
labels <- as.factor(dataset$popularity)
labels <- labels[-which(dataset$n_unique_tokens > 1)]
real_train_pred <- knn(train=features_clean[1:29999,], test = features_clean[30000:39643,], cl = labels[1:29999], k=20)
final <- read.csv("/Users/guglielmo/Desktop/Final/final.csv", header = TRUE, sep = ",")
sum(real_train_pred == final )
sum(real_train_pred == final$popularity )
sum(real_train_pred == final$popularity )/9644
View(features_clean)
if(!require("class")) install.packages("class"); library(class)
if(!require("HotDeckImputation")) install.packages("HotDeckImputation"); library(HotDeckImputation)
#get the data
train <- read.csv("../DATA/news_popularity_training.csv", sep = ",")
test <- read.csv("../DATA/news_popularity_test.csv", sep = ",")
test$popularity <- NA
dataset <- rbind(train,test)
features <- dataset[,c(2,4:61)]
labels <- as.factor(dataset$popularity)
three.cat <- function(x){
for(i in 1:length(x)){
if(x[i] > 2) x[i] <- 2
}
return(x)
}
features$num_imgs <- three.cat(features$num_imgs)
features$num_videos <- three.cat(features$num_videos)
features$n_non_stop_words <- NULL
# Remove the rate negative_words
features$rate_negative_words <- NULL
features$n_unique_tokens[features$n_tokens_content == 0] <- NA
features$n_non_stop_unique_tokens[features$n_tokens_content == 0] <- NA
features$num_hrefs[features$n_tokens_content == 0] <- NA
features$num_self_hrefs[features$n_tokens_content == 0] <- NA
features$average_token_length[features$n_tokens_content == 0] <- NA
features$n_tokens_content[features$n_tokens_content == 0] <- NA
# Recode the missing values
features$global_sentiment_polarity[features$global_subjectivity == 0] <- NA
features$global_rate_positive_words[features$global_subjectivity == 0] <- NA
features$global_rate_negative_words[features$global_subjectivity == 0] <- NA
features$rate_positive_words[features$global_subjectivity == 0] <- NA
features$avg_positive_polarity[features$global_subjectivity == 0] <- NA
features$avg_negative_polarity[features$global_subjectivity == 0] <- NA
features$min_positive_polarity[features$global_subjectivity == 0] <- NA
features$min_negative_polarity[features$global_subjectivity == 0] <- NA
features$max_positive_polarity[features$global_subjectivity == 0] <- NA
features$max_negative_polarity[features$global_subjectivity == 0] <- NA
features$global_subjectivity[features$global_subjectivity == 0] <- NA
imp.features <- impute.NN_HD(DATA=features[,-1],distance="eukl")
imp.features <- data.frame(url = features$url, imp.features)
obtain.date <- function(dataset){
dates = c("2013-01-01","2013-01-21","2013-02-18" , "2013-05-27", "2013-07-04",
"2013-09-02", "2013-10-14", "2013-11-11", "2013-11-28",
"2013-12-25", "2014-01-01", "2014-01-20", "2014-02-17", "2014-05-26",
"2014-07-04", "2014-09-01", "2014-10-13", "2014-11-11", "2014-11-27" ,
"2014-12-25")
myholidays  <- as.Date(dates,format ="%Y-%m-%d")
year <- as.numeric(substring(dataset$url, 21,24))
month <- as.numeric(substring(dataset$url, 26,27))
day <- as.numeric(substring(dataset$url, 29,30))
date <- paste(year,month,day, sep = "-")
date <- as.Date(date)
is_holiday <- rep(0,length(year))
is_holiday[which(date %in% myholidays)] <- 1
a <- data.frame(year = year,month = month, day = day, date = as.character(date), is_holiday = as.numeric(is_holiday))
return(a)
}
#obtain dates for training set
obtained.info <- obtain.date(imp.features)
is_holiday = obtained.info$is_holiday)
imp.features <- data.frame(imp.features, year = obtained.info$year,month = obtained.info$month,
is_holiday = obtained.info$is_holiday)
imp.features$url <- NULL
standardize <- function(x) {
num <- x - mean(x)
denom <-sd(x)
return (num/denom)
}
features.con <- apply(imp.features,2,standardize)
features_clean <- features.con
real_train_pred <- knn(train=features_clean[1:30000,], test = features_clean[30001:39643,], cl = labels[1:30000], k=20)
real_train_pred <- as.data.frame(real_train_pred)
View(real_train_pred)
table(real_train_pred)
final <- read.csv("/Users/guglielmo/Desktop/Final/final.csv", header = TRUE, sep = ",")
table(final)
table(final$popularity)
table(real_train_pred)
sum(real_train_pred == final$popularity[-9644] )/9643
features_clean = imp.features
real_train_pred <- knn(train=features_clean[1:30000,], test = features_clean[30001:39643,], cl = labels[1:30000], k=20)
real_train_pred <- as.data.frame(real_train_pred)
sum(real_train_pred == final$popularity[-9644] )/9643
