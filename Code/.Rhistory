dates = c("2013-01-01","2013-01-21","2013-02-18" , "2013-05-27", "2013-07-04",
"2013-09-02", "2013-10-14", "2013-11-11", "2013-11-28",
"2013-12-25", "2014-01-01", "2014-01-20", "2014-02-17", "2014-05-26",
"2014-07-04", "2014-09-01", "2014-10-13", "2014-11-11", "2014-11-27" ,
"2014-12-25")
myholidays  <- as.Date(dates,format ="%Y-%m-%d")
year <- as.numeric(substring(dataset$url, 21,24))
month <- as.numeric(substring(dataset$url, 26,27))
day <- as.numeric(substring(dataset$url, 29,30))
date <- paste(year,month,day, sep = "-")
date <- as.Date(date)
is_holiday <- rep(0,length(year))
is_holiday[which(date %in% myholidays)] <- 1
a <- data.frame(year = year,month = month, day = day, date = as.character(date), is_holiday = as.numeric(is_holiday))
return(a)
}
#obtain dates for training set
obtained.info <- obtain.date(imp.features)
#append the new created features
imp.features <- data.frame(imp.features, year = obtained.info$year,month = obtained.info$month,
is_holiday = obtained.info$is_holiday)
imp.features$url <- NULL
######Standardization of the features
#standardize the continuous features
standardize <- function(x) {
num <- x - mean(x)
denom <-sd(x)
return (num/denom)
}
#categorical standardization
cat_stand <- function(x){
num <- x - min(x)
denom <- max(x)-min(x)
return(num/denom)
}
#apply the changes (to be corrected)
features.con <- apply(imp.features[,c(1:6,9:10,17:28,37:56)],2,standardize)
features.dis <- apply(imp.features[,-c(1:6,9:10,17:28,37:56)],2,cat_stand)
features_clean <- data.frame(features.con, features.dis)
#real_train_pred <- knn(train=features_clean[1:29999,], test = features_clean[30000:39643,], cl = labels[1:29999], k=20)
#real_train_pred <- knn(train=features_clean[1:30000,], test = features_clean[30001:39643,], cl = labels[1:30000], k=20)
#real_train_pred <- as.data.frame(real_train_pred)
#final <- read.csv("/Users/guglielmo/Desktop/final_competition/final.csv", header = TRUE, sep = ",")
#sum(real_train_pred == final$popularity )/9644
####################
#FEATURE SELECTION
train.clean <- data.frame(features_clean,popularity = as.numeric(dataset$popularity))[1:30000,]
test.clean <- as.matrix(features_clean[-c(1:30000),])
X <- as.matrix(train.clean[,-60])
y <- as.factor(train.clean$popularity)
cvfit = cv.glmnet(X, y, family="multinomial", type.multinomial = "grouped", dfmax = 20)
rankvar = data.frame(as.matrix(coef(cvfit, s = "lambda.min")[[1]]))
topvar = data.frame(Variable = row.names(rankvar), coef = abs(rankvar$X1))
topvar = topvar[which(topvar$coef >0),]
topvar = topvar[-1,]
train.clean = train.clean[,which(colnames(train.clean) %in% c(topvar,"popularity"))]
test.clean = features_clean[30001:39644,which(colnames(train.clean) %in% topvar)]
View(test)
test.clean = features_clean[30001:39644,which(colnames(train.clean) %in% topvar)]
test.clean = features_clean[30001:39644,which(colnames(test.clean) %in% topvar)]
View(test.clean)
test.clean = features_clean[30001:39644,which(colnames(test) %in% topvar)]
View(topvar)
topvar = data.frame(Variable = row.names(rankvar), coef = abs(rankvar$X1))
topvar = topvar[which(topvar$coef >0),]
topvar = topvar[-1,]
topvar
topvar = topvar[1,]
rankvar = data.frame(as.matrix(coef(cvfit, s = "lambda.min")[[1]]))
topvar = data.frame(Variable = row.names(rankvar), coef = abs(rankvar$X1))
topvar = topvar[which(topvar$coef >0),]
topvar = topvar[-1,]
topvar = topvar[,1]
train.clean = train.clean[,which(colnames(train.clean) %in% c(topvar,"popularity"))]
topvar
topvar = as.numeric(topvar[,1])
topvar = topvar[,1]
topvar
train.clean = train.clean[,which(colnames(train.clean) %in% c(topvar,"popularity"))]
test.clean = features_clean[30001:39644,which(colnames(test) %in% topvar)]
View(test.clean)
test.clean = features_clean[30001:39644,which(colnames(test) %in% topvar)]
train.clean <- data.frame(features_clean,popularity = as.numeric(dataset$popularity))[1:30000,]
train.clean = train.clean[,which(colnames(train.clean) %in% c(topvar,"popularity"))]
topvar = as.character(topvar[,1])
topvar = as.character(topvar[,1])
topvar = as.character(topvar)
train.clean = train.clean[,which(colnames(train.clean) %in% c(topvar,"popularity"))]
train.clean <- data.frame(features_clean,popularity = as.numeric(dataset$popularity))[1:30000,]
train.clean = train.clean[,which(colnames(train.clean) %in% c(topvar,"popularity"))]
View(train.clean)
train.clean <- data.frame(features_clean,popularity = as.numeric(dataset$popularity))[1:30000,]
test.clean <- as.matrix(features_clean[-c(1:30000),])
train.clean <- data.frame(features_clean,popularity = as.numeric(dataset$popularity))[1:30000,]
train.clean = train.clean[,which(colnames(train.clean) %in% c(topvar,"popularity"))]
test.clean = features_clean[30001:39644,which(colnames(test) %in% topvar)]
train.clean <- data.frame(features_clean,popularity = as.numeric(dataset$popularity))[1:30000,]
test.clean <- as.matrix(features_clean[-c(1:30000),])
train.clean = train.clean[,which(colnames(train.clean) %in% c(topvar,"popularity"))]
test.clean = features_clean[30001:39644,which(colnames(test.clean) %in% topvar)]
knn(train=train.clean[,-21], test = test.clean, cl = train.clean$popularity, k=20)
real <- knn(train=train.clean[,-21], test = test.clean, cl = train.clean$popularity, k=20)
table(real)
sum(real == final$popularity )/9644
final <- read.csv("/Users/guglielmo/Desktop/final_competition/final.csv", header = TRUE, sep = ",")
sum(real == final$popularity )/9644
train.clean <- data.frame(features_clean,popularity = as.numeric(dataset$popularity))[1:30000,]
#Split into popular and not popular
train.clean$dummy[train.clean$popularity < 2] <- 0
train.clean$dummy[train.clean$popularity > 1] <- 1
fisher.rank <- function(feature,label){
num <- (mean(feature[label == 0]) - mean(feature[label == 1]))^2
denom <- var(feature[label == 0]) + var(feature[label == 1])
rank <- round(num/denom,4)
return(rank)
}
X = train.clean[,-which(colnames(train.clean) %in% c("popularity","dummy"))]
fisher.score = apply(X,2,function(x)fisher.rank(x,train.clean$dummy))
names(fisher.score) = colnames(X)
top.ranks = fisher.score[order(fisher.score,decreasing = T)]
top.vars = names(top.ranks[1:40])
train.clean = train.clean[,which(colnames(train.clean) %in% c(top.vars,"popularity"))]
test.clean = features_clean[30001:39644,which(colnames(train.clean) %in% top.vars)]
real_train_pred <- knn(train=train.clean[,-41], test = test.clean, cl = train.clean$popularity, k=20)
real_train_pred <- as.data.frame(real_train_pred)
final <- read.csv("/Users/guglielmo/Desktop/final_competition/final.csv", header = TRUE, sep = ",")
sum(real_train_pred == final$popularity )/9644
install.packages("libsvm")
install.packages("e1071")
library("e1071")
?svm
library(randomForest)
library(RandomForest)
install.packages("randomForest")
library(randomForest)
library(e1071)
?randomForest
?svm
View(train)
features_clean <- data.frame(features.con, features.dis)
train.clean <- data.frame(features_clean,popularity = as.numeric(dataset$popularity))[1:30000,]
test.clean = features_clean[30001:39644,]
model <- svm( train.clean$popularity~., train.clean )
res <- predict( model, newdata=test.clean )
head(res)
round(res)
fit <- round(res)
sum(fit == final$popularity )/9644
train <- read.csv("../DATA/news_popularity_training.csv", sep = ",")
test <- read.csv("../DATA/news_popularity_test.csv", sep = ",")
proximity=TRUE,importance=TRUE)
model <- svm( train$popularity~., train )
res2 <- predict(model, newdata=test)
res2 <- predict(model, newdata=test)
res <- predict( model, newdata=test.clean )
res2 <- predict(model, newdata=test)
res2 <- predict(model, newdata=test[1:1000,])
table(train$popularity)
View(train)
svm <- svm(x = train[,1:61], y = train[,62],
xtest = test,  kernel ="polynomial")
if(!require("class")) install.packages("class"); library(class)
if(!require("HotDeckImputation")) install.packages("HotDeckImputation"); library(HotDeckImputation)
#install.packages("glmnet")
#install.packages("lars")
#install.packages("e1071")
#install.packages("randomForest")
library(randomForest)
library(e1071)
library(glmnet)
library(lars)
#get the data
train <- read.csv("../DATA/news_popularity_training.csv", sep = ",")
test <- read.csv("../DATA/news_popularity_test.csv", sep = ",")
test$popularity <- NA
dataset <- rbind(train,test)
features <- dataset[,-which(colnames(dataset) %in% c("popularity"))]
###############################################################
##DATA MANIPULATION & CLEANING
#transform number of images & videos into 3-categorical variables (0, 1 or more than 1)
three.cat <- function(x){
for(i in 1:length(x)){
if(x[i] > 2) x[i] <- 2
}
return(x)
}
features$num_imgs <- three.cat(features$num_imgs)
features$num_videos <- three.cat(features$num_videos)
#Remove non-sense or redundant features:
# Remove the constant column
features$n_non_stop_words <- NULL
# Remove the rate negative_words
features$rate_negative_words <- NULL
# # Remove the ukrain outlier
# features <- features[-which(features$n_unique_tokens > 1),]
# labels <- labels[-which(dataset$n_unique_tokens > 1)]
# Recode the missing values
features$n_unique_tokens[features$n_tokens_content == 0] <- NA
features$n_non_stop_unique_tokens[features$n_tokens_content == 0] <- NA
features$num_hrefs[features$n_tokens_content == 0] <- NA
features$num_self_hrefs[features$n_tokens_content == 0] <- NA
features$average_token_length[features$n_tokens_content == 0] <- NA
features$n_tokens_content[features$n_tokens_content == 0] <- NA
# Recode the missing values
features$global_sentiment_polarity[features$global_subjectivity == 0] <- NA
features$global_rate_positive_words[features$global_subjectivity == 0] <- NA
features$global_rate_negative_words[features$global_subjectivity == 0] <- NA
features$rate_positive_words[features$global_subjectivity == 0] <- NA
features$avg_positive_polarity[features$global_subjectivity == 0] <- NA
features$avg_negative_polarity[features$global_subjectivity == 0] <- NA
features$min_positive_polarity[features$global_subjectivity == 0] <- NA
features$min_negative_polarity[features$global_subjectivity == 0] <- NA
features$max_positive_polarity[features$global_subjectivity == 0] <- NA
features$max_negative_polarity[features$global_subjectivity == 0] <- NA
features$global_subjectivity[features$global_subjectivity == 0] <- NA
# Hot deck Imputation
imp.features <- impute.NN_HD(DATA=features[,-c(1:3)],distance="eukl")
imp.features <- data.frame(url = features$url, imp.features)
#obtain date & holiday variables
#New years, Martin Luther, Washington's Birthday, Memorial day, Independence, labor, Columbus
#Veterans, Thanksgiving, Christmas
obtain.date <- function(dataset){
dates = c("2013-01-01","2013-01-21","2013-02-18" , "2013-05-27", "2013-07-04",
"2013-09-02", "2013-10-14", "2013-11-11", "2013-11-28",
"2013-12-25", "2014-01-01", "2014-01-20", "2014-02-17", "2014-05-26",
"2014-07-04", "2014-09-01", "2014-10-13", "2014-11-11", "2014-11-27" ,
"2014-12-25")
myholidays  <- as.Date(dates,format ="%Y-%m-%d")
year <- as.numeric(substring(dataset$url, 21,24))
month <- as.numeric(substring(dataset$url, 26,27))
day <- as.numeric(substring(dataset$url, 29,30))
date <- paste(year,month,day, sep = "-")
date <- as.Date(date)
is_holiday <- rep(0,length(year))
is_holiday[which(date %in% myholidays)] <- 1
a <- data.frame(year = year,month = month, day = day, date = as.character(date), is_holiday = as.numeric(is_holiday))
return(a)
}
#obtain dates for training set
obtained.info <- obtain.date(imp.features)
#append the new created features
imp.features <- data.frame(imp.features, year = obtained.info$year,month = obtained.info$month,
is_holiday = obtained.info$is_holiday)
imp.features$url <- NULL
######Standardization of the features
#standardize the continuous features
standardize <- function(x) {
num <- x - mean(x)
denom <-sd(x)
return (num/denom)
}
#categorical standardization
cat_stand <- function(x){
num <- x - min(x)
denom <- max(x)-min(x)
return(num/denom)
}
#apply the changes (to be corrected)
features.con <- apply(imp.features[,c(1:6,9:10,17:28,37:56)],2,standardize)
features.dis <- apply(imp.features[,-c(1:6,9:10,17:28,37:56)],2,cat_stand)
features_clean <- data.frame(features.con, features.dis)
#real_train_pred <- knn(train=features_clean[1:29999,], test = features_clean[30000:39643,], cl = labels[1:29999], k=20)
#real_train_pred <- knn(train=features_clean[1:30000,], test = features_clean[30001:39643,], cl = labels[1:30000], k=20)
#real_train_pred <- as.data.frame(real_train_pred)
#final <- read.csv("/Users/guglielmo/Desktop/final_competition/final.csv", header = TRUE, sep = ",")
#sum(real_train_pred == final$popularity )/9644
####################
#FEATURE SELECTION
train.clean <- data.frame(features_clean,popularity = as.numeric(dataset$popularity))[1:30000,]
res2 <- predict(model, newdata=as.matrix(test))
model <- svm( train$popularity~., train )
res2 <- predict(model, newdata=as.matrix(test))
test$popularity <- NA
View(test)
res2 <- predict(model, newdata=as.matrix(test))
str(test)
str(train)
View(train)
train <- train[,3:62]
test <- test[, 3:61]
model <- svm( train$popularity~., train )
res2 <- predict(model, newdata=as.matrix(test))
fit2 <- signif(res2, digits = 1)
sum(fit == final$popularity)/9644
sum(fit2 == final$popularity)/9644
final <- read.csv("/Users/guglielmo/Desktop/final_competition/final.csv", header = TRUE, sep = ",")
sum(fit2 == final$popularity)/9644
fit3 <- round(res2)
sum(fit3 == final$popularity)/9644
test.clean = features_clean[30001:39644,]
test.clean <- as.matrix(features_clean[-c(1:30000),])
test.clean <- as.matrix(features_clean[-c(1:30000),])
model0 <- svm( train.clean$popularity~., train.clean )
res <- predict( model0, newdata=test.clean )
fit <- signif(res, digits = 1)
sum(fit == final$popularity )/9644
fit1 <- round(res)
sum(fit1 == final$popularity )/9644
train.clean <- data.frame(features_clean,popularity = as.numeric(dataset$popularity))[1:30000,]
test.clean <- as.matrix(features_clean[-c(1:30000),])
X <- as.matrix(train.clean[,-60])
y <- as.factor(train.clean$popularity)
cvfit = cv.glmnet(X, y, family="multinomial", type.multinomial = "grouped", dfmax = 20)
rankvar = data.frame(as.matrix(coef(cvfit, s = "lambda.min")[[1]]))
topvar = data.frame(Variable = row.names(rankvar), coef = abs(rankvar$X1))
topvar = topvar[which(topvar$coef >0),]
topvar = topvar[-1,]
topvar = as.character(topvar)
train.clean = train.clean[,which(colnames(train.clean) %in% c(topvar,"popularity"))]
test.clean = features_clean[30001:39644,which(colnames(test.clean) %in% topvar)]
topvar
rankvar = data.frame(as.matrix(coef(cvfit, s = "lambda.min")[[1]]))
rankvar
topvar = data.frame(Variable = row.names(rankvar), coef = abs(rankvar$X1))
View(topvar)
topvar = topvar[which(topvar$coef >0),]
topvar = topvar[-1,]
View(topvar)
topvar = as.character(topvar)
rankvar = data.frame(as.matrix(coef(cvfit, s = "lambda.min")[[1]]))
topvar = data.frame(Variable = row.names(rankvar), coef = abs(rankvar$X1))
topvar = topvar[which(topvar$coef >0),]
topvar = topvar[-1,]
topvar = as.character(topvar[,1])
train.clean = train.clean[,which(colnames(train.clean) %in% c(topvar,"popularity"))]
train.clean <- data.frame(features_clean,popularity = as.numeric(dataset$popularity))[1:30000,]
train.clean = train.clean[,which(colnames(train.clean) %in% c(topvar,"popularity"))]
test.clean = features_clean[30001:39644,which(colnames(test.clean) %in% topvar)]
test.clean = features_clean[30001:39644,]
test.clean = features_clean[30001:39644,which(colnames(test.clean) %in% topvar)]
model3 <- svm(train.clean$popularity~., train.clean)
res4 <- predict(model3, newdata = as.matrix(test.clean))
fit4 <- signif(res4, digits = 1)
fit5 <- round(res4)
sum(fit4 == final$popularity)/9644
sum(fit5 == final$popularity)/9644
if(!require("class")) install.packages("class"); library(class)
if(!require("HotDeckImputation")) install.packages("HotDeckImputation"); library(HotDeckImputation)
#install.packages("glmnet")
#install.packages("lars")
#install.packages("e1071")
#install.packages("randomForest")
library(randomForest)
library(e1071)
library(glmnet)
library(lars)
#get the data
train <- read.csv("../DATA/news_popularity_training.csv", sep = ",")
test <- read.csv("../DATA/news_popularity_test.csv", sep = ",")
test$popularity <- NA
dataset <- rbind(train,test)
features <- dataset[,-which(colnames(dataset) %in% c("popularity"))]
###############################################################
##DATA MANIPULATION & CLEANING
#transform number of images & videos into 3-categorical variables (0, 1 or more than 1)
three.cat <- function(x){
for(i in 1:length(x)){
if(x[i] > 2) x[i] <- 2
}
return(x)
}
features$num_imgs <- three.cat(features$num_imgs)
features$num_videos <- three.cat(features$num_videos)
#Remove non-sense or redundant features:
# Remove the constant column
features$n_non_stop_words <- NULL
# Remove the rate negative_words
features$rate_negative_words <- NULL
# # Remove the ukrain outlier
# features <- features[-which(features$n_unique_tokens > 1),]
# labels <- labels[-which(dataset$n_unique_tokens > 1)]
# Recode the missing values
features$n_unique_tokens[features$n_tokens_content == 0] <- NA
features$n_non_stop_unique_tokens[features$n_tokens_content == 0] <- NA
features$num_hrefs[features$n_tokens_content == 0] <- NA
features$num_self_hrefs[features$n_tokens_content == 0] <- NA
features$average_token_length[features$n_tokens_content == 0] <- NA
features$n_tokens_content[features$n_tokens_content == 0] <- NA
# Recode the missing values
features$global_sentiment_polarity[features$global_subjectivity == 0] <- NA
features$global_rate_positive_words[features$global_subjectivity == 0] <- NA
features$global_rate_negative_words[features$global_subjectivity == 0] <- NA
features$rate_positive_words[features$global_subjectivity == 0] <- NA
features$avg_positive_polarity[features$global_subjectivity == 0] <- NA
features$avg_negative_polarity[features$global_subjectivity == 0] <- NA
features$min_positive_polarity[features$global_subjectivity == 0] <- NA
features$min_negative_polarity[features$global_subjectivity == 0] <- NA
features$max_positive_polarity[features$global_subjectivity == 0] <- NA
features$max_negative_polarity[features$global_subjectivity == 0] <- NA
features$global_subjectivity[features$global_subjectivity == 0] <- NA
# Hot deck Imputation
imp.features <- impute.NN_HD(DATA=features[,-c(1:3)],distance="eukl")
imp.features <- data.frame(url = features$url, imp.features)
#obtain date & holiday variables
#New years, Martin Luther, Washington's Birthday, Memorial day, Independence, labor, Columbus
#Veterans, Thanksgiving, Christmas
obtain.date <- function(dataset){
dates = c("2013-01-01","2013-01-21","2013-02-18" , "2013-05-27", "2013-07-04",
"2013-09-02", "2013-10-14", "2013-11-11", "2013-11-28",
"2013-12-25", "2014-01-01", "2014-01-20", "2014-02-17", "2014-05-26",
"2014-07-04", "2014-09-01", "2014-10-13", "2014-11-11", "2014-11-27" ,
"2014-12-25")
myholidays  <- as.Date(dates,format ="%Y-%m-%d")
year <- as.numeric(substring(dataset$url, 21,24))
month <- as.numeric(substring(dataset$url, 26,27))
day <- as.numeric(substring(dataset$url, 29,30))
date <- paste(year,month,day, sep = "-")
date <- as.Date(date)
is_holiday <- rep(0,length(year))
is_holiday[which(date %in% myholidays)] <- 1
a <- data.frame(year = year,month = month, day = day, date = as.character(date), is_holiday = as.numeric(is_holiday))
return(a)
}
#obtain dates for training set
obtained.info <- obtain.date(imp.features)
#append the new created features
imp.features <- data.frame(imp.features, year = obtained.info$year,month = obtained.info$month,
is_holiday = obtained.info$is_holiday)
imp.features$url <- NULL
######Standardization of the features
#standardize the continuous features
standardize <- function(x) {
num <- x - mean(x)
denom <-sd(x)
return (num/denom)
}
#categorical standardization
cat_stand <- function(x){
num <- x - min(x)
denom <- max(x)-min(x)
return(num/denom)
}
#apply the changes (to be corrected)
features.con <- apply(imp.features[,c(1:6,9:10,17:28,37:56)],2,standardize)
features.dis <- apply(imp.features[,-c(1:6,9:10,17:28,37:56)],2,cat_stand)
features_clean <- data.frame(features.con, features.dis)
#real_train_pred <- knn(train=features_clean[1:29999,], test = features_clean[30000:39643,], cl = labels[1:29999], k=20)
#real_train_pred <- knn(train=features_clean[1:30000,], test = features_clean[30001:39643,], cl = labels[1:30000], k=20)
#real_train_pred <- as.data.frame(real_train_pred)
#final <- read.csv("/Users/guglielmo/Desktop/final_competition/final.csv", header = TRUE, sep = ",")
#sum(real_train_pred == final$popularity )/9644
####################
#FEATURE SELECTION
train.clean <- data.frame(features_clean,popularity = as.numeric(dataset$popularity))[1:30000,]
#Split into popular and not popular
train.clean$dummy[train.clean$popularity < 2] <- 0
train.clean$dummy[train.clean$popularity > 1] <- 1
fisher.rank <- function(feature,label){
num <- (mean(feature[label == 0]) - mean(feature[label == 1]))^2
denom <- var(feature[label == 0]) + var(feature[label == 1])
rank <- round(num/denom,4)
return(rank)
}
X = train.clean[,-which(colnames(train.clean) %in% c("popularity","dummy"))]
fisher.score = apply(X,2,function(x)fisher.rank(x,train.clean$dummy))
names(fisher.score) = colnames(X)
top.ranks = fisher.score[order(fisher.score,decreasing = T)]
top.vars = names(top.ranks[1:40])
train.clean = train.clean[,which(colnames(train.clean) %in% c(top.vars,"popularity"))]
test.clean = features_clean[30001:39644,which(colnames(train.clean) %in% top.vars)]
model4 <- svm(train.clean$popularity~., train.clean)
res5 <- predict(model4, newdata = as.matrix(test.clean))
res5 <- predict(model4, newdata = as.matrix(test.clean))
View(test.clean)
test.clean = features_clean[30001:39644,which(colnames(test.clean) %in% top.vars)]
test.clean <- features_clean[-1:30000,]
test.clean <- features_clean[-c(1:30000),]
train.clean$dummy[train.clean$popularity < 2] <- 0
train.clean$dummy[train.clean$popularity > 1] <- 1
fisher.rank <- function(feature,label){
num <- (mean(feature[label == 0]) - mean(feature[label == 1]))^2
denom <- var(feature[label == 0]) + var(feature[label == 1])
rank <- round(num/denom,4)
return(rank)
}
X = train.clean[,-which(colnames(train.clean) %in% c("popularity","dummy"))]
fisher.score = apply(X,2,function(x)fisher.rank(x,train.clean$dummy))
names(fisher.score) = colnames(X)
top.ranks = fisher.score[order(fisher.score,decreasing = T)]
top.vars = names(top.ranks[1:40])
train.clean = train.clean[,which(colnames(train.clean) %in% c(top.vars,"popularity"))]
test.clean = features_clean[30001:39644,which(colnames(test.clean) %in% top.vars)]
res5 <- predict(model4, newdata = as.matrix(test.clean))
fit6 <- signif(res5, digits = 1)
fit7 <- round(res5)
sum(fit6 == final$popularity)/9644
final <- read.csv("/Users/guglielmo/Desktop/final_competition/final.csv", header = TRUE, sep = ",")
sum(fit6 == final$popularity)/9644
sum(fit7 == final$popularity)/9644  #50,63252%
sum(fit5 == final$popularity)/9644 #50,something %
setwd("~/Desktop/BGSE/Kaggle_Competition/Code")
train <- read.csv("../DATA/news_popularity_training.csv", sep = ",")
test <- read.csv("../DATA/news_popularity_test.csv", sep = ",")
final <- read.csv("/Users/guglielmo/Desktop/final_competition/final.csv", header = TRUE, sep = ",")
install.packages("rpartScore")
library(rpartScore)
model <- rpart( train$popularity~., train )
res2 <- predict(model, newdata=as.matrix(test))
res2 <- predict(model, newdata=test)
train <- train[,3:62]
test <- test[, 3:61]
model <- rpart( train$popularity~., train )
res2 <- predict(model, newdata=test)
head(res2)
fit1 <- round(res2)
sum(fit1 == final$popularity )/9644
fit2 <- signif(res2)
sum(fit2 == final$popularity)/9644
fit2 <- signif(res2, digits = 1)
sum(fit1 == final$popularity )/9644  #0.4697221
sum(fit2 == final$popularity)/9644
install.packages("rpartOrdinal")
